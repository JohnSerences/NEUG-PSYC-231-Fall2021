{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 06 in class exercises (randomization and bootstrapping)\n",
    "## Goals\n",
    "* Practice working through coding basics on your own\n",
    "* Get a better intuition about when standard parametric t-tests and non-parametric approaches produce similar results and when they can diverge\n",
    "* See the importance of plotting your data before you do anything else!\n",
    "* Apply bootstrapping to some real EEG data to estimate confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First start by importing the packages you'll need. \n",
    "* Numpy, scipy, and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats  # has t-tests and other stats stuff...\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Start with some data.\n",
    "* First plot it\n",
    "* Then compute summary stats (mean, std) for each data array\n",
    "* Then compute the correlation coeffecient that relates the two arrays\n",
    "* Then the standard parametric t-value and p-value associated with the correlation. \n",
    "* Try to do this without copying from the in-class tutorial! Google the formulas if you don't remember them and then try to translate them into python (you can peek at in class tutorial if you get stuck, but its good practice to just hack it out)\n",
    "\n",
    "[source of this famous data set: Anscombe](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\n",
    "d2 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data...scatter works well here\n",
    "* First rule of data analysis...always plot your data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot the data! always do this first (have we said that already???)\n",
    "plt.scatter(d1,d2,color='r',marker='o',linewidths=3)\n",
    "plt.xlabel('Data set 1', **fig_font)\n",
    "plt.ylabel('Data set 2', **fig_font)\n",
    "plt.xticks(**fig_font)\n",
    "plt.yticks(**fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mean, std of both data arrays\n",
    "* Leave the output in the notebook so that we can come back and compare later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean, std for the first data set\n",
    "mean_data = np.mean(d1, axis=0)\n",
    "std_data = np.std(d1, axis=0)\n",
    "print('mean: ', mean_data, ' std: ', std_data)\n",
    "\n",
    "# compute mean, std for the second data set\n",
    "mean_data = np.mean(d2, axis=0)\n",
    "std_data = np.std(d2, axis=0)\n",
    "print('mean of 2nd: ', mean_data, ' std of 2nd: ', std_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate the two data arrays, compute t-value and p-value associated with correlation coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation coeffecient\n",
    "N = len(d1)\n",
    "\n",
    "corr = np.corrcoef(d1,d2)[0,1]\n",
    "print('Correlation coef: ', corr)\n",
    "\n",
    "# compute the t-value/p-value corresponding to the correlation\n",
    "t_val = (corr*np.sqrt(N-2)) / np.sqrt(1-corr**2)\n",
    "p_value = 2*(1-stats.t.cdf(t_val, N-1))\n",
    "print('T value: ', t_val, 'P Value: ', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use randomization testing to eval the reliability of the estimated p-value. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Before you run this procedure, you should have a pretty good idea about how the standard p-value will compare with the p-value that you estimate using randomization. Make a prediction!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many times we want to run the randomization test...\n",
    "num_randomizations = 1000\n",
    "rand_corr = np.zeros(num_randomizations)\n",
    "rand_t_val = np.zeros(num_randomizations)\n",
    "tmp0 = np.zeros(N)\n",
    "tmp1 = np.zeros(N)\n",
    "\n",
    "# start a loop over randomization iterations\n",
    "for i in np.arange(num_randomizations):\n",
    "    \n",
    "    # write this out explicitly for clarity - randomly assigning numbers from d1 or d2\n",
    "    # i.e. condition doesn't matter. \n",
    "    for j in np.arange(N):   \n",
    "        if np.random.rand(1) < .5:\n",
    "            tmp0[j] = d1[j]\n",
    "            tmp1[j] = d2[j]\n",
    "        else:\n",
    "            tmp0[j] = d2[j]\n",
    "            tmp1[j] = d1[j]\n",
    "\n",
    "    # then correlate the two randomized data vectors...compute tvalues\n",
    "    rand_corr[i] = np.corrcoef(tmp0,tmp1)[0,1]\n",
    "    rand_t_val[i] = (rand_corr[i]*np.sqrt(N-2)) / np.sqrt(1-rand_corr[i]**2)\n",
    "\n",
    "# compute the p-value of our real t-score (t_val) vs our radomized distribution\n",
    "rand_p_value = 2*(1-(np.sum(t_val>rand_t_val) / num_randomizations))\n",
    "print('Randomization-based p-value: ', np.round(rand_p_value,8), ' Parametric P-value: ', np.round(p_value,8))\n",
    "\n",
    "# plotting the distribution of p-values that we observe under the null\n",
    "plt.hist(rand_t_val, color='r', alpha=1, bins=30)\n",
    "plt.xlabel('T-value under null')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Here is a second data set (actually, its another part of the Anscombe data set...but lets pretend like its an entirely new data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\n",
    "d2 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the first rule of data analysis, and BEFORE you plot the data, compute the mean and std of these two arrays\n",
    "* What do you notice when you compare them to the mean and std of the arrays in the first part of the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean, std for the first data set\n",
    "mean_data = np.mean(d1, axis=0)\n",
    "std_data = np.std(d1, axis=0)\n",
    "print('mean: ', mean_data, ' std: ', std_data)\n",
    "\n",
    "# compute mean, std for the second data set\n",
    "mean_data = np.mean(d2, axis=0)\n",
    "std_data = np.std(d2, axis=0)\n",
    "print('mean of 2nd: ', mean_data, ' std of 2nd: ', std_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based only on your comparison of the mean and std of the data from Part I and Part II, what is your prediction about the effects of randomization testing on this new data set? \n",
    "* Will the randomization based p-value be similar to the standard p-value as it was in the example above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok - now plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot the data! always do this first\n",
    "plt.scatter(d1,d2,color='r',marker='o',linewidths=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on this, what do you think is going to happen when we compare the output from a parametric test and a radomization test?\n",
    "* Remember that the mean and the std of the data sets in Part I and Part II are identical...\n",
    "* Compute correlation coef, and do randomization testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation coeffecient\n",
    "N = len(d1)\n",
    "\n",
    "corr = np.corrcoef(d1,d2)[0,1]\n",
    "print('Correlation coef: ', corr)\n",
    "\n",
    "# compute the t-value/p-value corresponding to the correlation\n",
    "t_val = (corr*np.sqrt(N-2)) / np.sqrt(1-corr**2)\n",
    "p_value = 2*(1-stats.t.cdf(t_val, N-1))\n",
    "print('T value: ', t_val, 'P Value: ', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the correlation coeffecients to those from Part I.\n",
    "* So far, everything is pretty much the same between the data sets (same mean/std/corr coef)\n",
    "* So should randomization testing yield about the same results? Try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many times we want to run the randomization test...\n",
    "num_randomizations = 1000\n",
    "rand_corr = np.zeros(num_randomizations)\n",
    "rand_t_val = np.zeros(num_randomizations)\n",
    "tmp0 = np.zeros(N)\n",
    "tmp1 = np.zeros(N)\n",
    "\n",
    "# start a loop over randomization iterations\n",
    "for i in np.arange(num_randomizations):\n",
    "    \n",
    "    # write this out explicitly for clarity - randomly assigning numbers from d1 or d2\n",
    "    # i.e. condition doesn't matter. \n",
    "    for j in np.arange(N):   \n",
    "        if np.random.rand(1) < .5:\n",
    "            tmp0[j] = d1[j]\n",
    "            tmp1[j] = d2[j]\n",
    "        else:\n",
    "            tmp0[j] = d2[j]\n",
    "            tmp1[j] = d1[j]\n",
    "\n",
    "    # then correlate the two randomized data vectors...compute tvalues\n",
    "    rand_corr[i] = np.corrcoef(tmp0,tmp1)[0,1]\n",
    "    rand_t_val[i] = (rand_corr[i]*np.sqrt(N-2)) / np.sqrt(1-rand_corr[i]**2)\n",
    "\n",
    "# compute the p-value of our real t-score (t_val) vs our radomized distribution\n",
    "rand_p_value = 2*(1-(np.sum(t_val>rand_t_val) / num_randomizations))\n",
    "print('Randomization-based p-value: ', np.round(rand_p_value,8), ' Parametric P-value: ', np.round(p_value,8))\n",
    "\n",
    "# plotting the distribution of p-values that we observe under the null\n",
    "plt.hist(rand_t_val, color='r', alpha=1, bins=30)\n",
    "plt.xlabel('T-value under null')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Load in the second EEG data set from last week (eeg_data01.npz). \n",
    "* Pull out the data, sr, and tx arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data...\n",
    "eeg = np.load('eeg_data01.npz')\n",
    "\n",
    "# get the different arrays like this...kind of like a structure in matlab. \n",
    "eeg['data']\n",
    "eeg['sr']\n",
    "eeg['tx']\n",
    "\n",
    "# and can query the attributes of the data like this...which will tell us that there 1600 trials and 4102 timepoints per trial sampled\n",
    "# at 1024Hz\n",
    "print('Shape of the big eeg data set: ', eeg['data'].shape)\n",
    "print('Sample rate: ', eeg['sr'])\n",
    "\n",
    "# and if you want to save some typing, especially because we only have a few variables, you reassign the different arrays like this\n",
    "data = eeg['data']\n",
    "sr = eeg['sr']\n",
    "tx = eeg['tx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is a vector that labels each trial as coming from experimental conditions 1,2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data.shape[0]\n",
    "num_samp_cond = int(N/4)\n",
    "cond = np.hstack((np.ones(num_samp_cond), np.ones(num_samp_cond)*2, np.ones(num_samp_cond)*3, np.ones(num_samp_cond)*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now avgerage the data from condition 2, avg the data from condition 3, and plot against the time axis (tx) - we're going to ignore conditions 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg1 = np.mean(data[cond==2,:], axis=0)\n",
    "avg2 = np.mean(data[cond==3,:], axis=0)\n",
    "\n",
    "plt.plot(tx, avg1, color='r', linewidth=2)\n",
    "plt.plot(tx, avg2, color='k', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next do a t-test for related samples comparing the responses in conditions 2 and 3 at each point in time. Note - you can do this all in one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = stats.ttest_rel(data[cond==2,:],data[cond==3,:],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now plot the averages in one plot, and then right below that make another plot with the t-values. Keep in mind that with this many degrees of freedom, a t-value of approx 1.9 is significant at the magic 0.05 level (or 1.68 one-tailed). So put some horizontal lines on the plot at 1.9 and -1.9. You'll see some pretty impressive t-values in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg1 = np.mean(data[cond==2,:], axis=0)\n",
    "avg2 = np.mean(data[cond==3,:], axis=0)\n",
    "\n",
    "plt.plot(tx, avg1, color='r', linewidth=2)\n",
    "plt.plot(tx, avg2, color='k', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tx, t_test.statistic, color='k', linewidth=2)\n",
    "plt.axhline(1.9, color='k')\n",
    "plt.axhline(-1.9, color='k')\n",
    "plt.xlabel('Time->')\n",
    "plt.ylabel('T-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should see that there is a significant difference at many timepoints...Now figure out how robust those effects are by applying bootstrapping \n",
    "* To do this, you might first create two 800 x 4102 matrices, one with data from all trials of condition 2, and one with data from all trials of condition 3\n",
    "* Then resample 800 trials, with replacement, from each data matrix and then do the t-test. \n",
    "* try generating a set of 800 values with repeating numbers that you can use for a row index into the data matrices\n",
    "* repeat and then compute CIs of the t-value\n",
    "* how often do the CIs for the t-value overlap with 0???\n",
    "* note - this can take a while, so start with 50 bootstraps and then increase as compute time allows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data matrices\n",
    "d1 = data[cond==2,:]\n",
    "d2 = data[cond==3,:]\n",
    "\n",
    "# store the number of trials\n",
    "N = d1.shape[0]\n",
    "\n",
    "# number of bootstraps\n",
    "num_bootstraps = 50\n",
    "boot_t_val = np.zeros((num_bootstraps,data.shape[1]))\n",
    "\n",
    "# bootstrapping loop...\n",
    "for i in np.arange(num_bootstraps):\n",
    "    # with replacement generate a sample number from 0:N exclusive and do that N times\n",
    "    index1 = np.random.randint(num_samp_cond, size=num_samp_cond)\n",
    "    index2 = np.random.randint(num_samp_cond, size=num_samp_cond)\n",
    "\n",
    "    # use that to pull data from each of our arrays\n",
    "    tmp1 = d1[index1,:]\n",
    "    tmp2 = d2[index2,:]  \n",
    "    \n",
    "    # compute t-test between first two vectors. \n",
    "    # tmp_t_stats = stats.ttest_rel(tmp1, tmp2, axis=0)   # scipy way is slower\n",
    "    # boot_t_val[i,:] = tmp_t_stats.statistic\n",
    "    \n",
    "    # faster to write it out yourself\n",
    "    tmp_t_stats = np.mean(tmp1-tmp2, axis=0) / (np.std(tmp1-tmp2, axis=0) / np.sqrt(N-1))\n",
    "    boot_t_val[i,:] = tmp_t_stats\n",
    "\n",
    "\n",
    "# then compute 95% CIs based on percentiles \n",
    "CI = np.percentile(boot_t_val, [2.5, 97.5], axis=0)\n",
    "\n",
    "# plot\n",
    "plt.title('T-values values +- 95% CIs')\n",
    "plt.plot(tx, np.mean(boot_t_val, axis=0), color='k', linewidth=2)\n",
    "plt.plot(tx, CI[0,], color='r', linewidth=1)\n",
    "plt.plot(tx, CI[1,], color='r', linewidth=1)\n",
    "plt.axhline(0, color='k')\n",
    "plt.xlabel('Time->')\n",
    "plt.ylabel('T-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do things compare?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
